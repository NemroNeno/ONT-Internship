{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"Dataset_timeseries.csv\")\n",
    "data = df.groupby(\"Number\")\n",
    "split = {g: d for g, d in data}\n",
    "dataset = split[1].iloc[:, 1:]\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "sequences = dataset['SOPAS'].values.reshape(-1, 1)\n",
    "\n",
    "# Isolation Forest for outlier detection\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "model.fit(sequences)\n",
    "\n",
    "# Predict outliers\n",
    "predictions = model.predict(sequences)\n",
    "outliers = np.where(predictions == -1)\n",
    "outlier_data = sequences[outliers].reshape(-1, 1)\n",
    "un_transformed=outlier_data\n",
    "outlier_data = sc.fit_transform(outlier_data)\n",
    "normal_data = sequences[np.where(predictions == 1)].reshape(-1, 1)\n",
    "print(len(normal_data))\n",
    "\n",
    "# Define GAN components\n",
    "def build_generator(noise_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=noise_dim))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2048))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(1, activation='tanh')) \n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=1))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(2048))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Initialize models\n",
    "noise_dim = 200\n",
    "generator = build_generator(noise_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile models\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_step(real_samples):\n",
    "    batch_size = real_samples.shape[0]\n",
    "\n",
    "    # Generate fake samples\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    generated_samples = generator(noise, training=True)\n",
    "\n",
    "    # Real and fake labels\n",
    "    real_labels = tf.ones((batch_size, 1))\n",
    "    fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "    # Train discriminator\n",
    "    with tf.GradientTape() as tape:\n",
    "        d_loss_real = discriminator(real_samples, training=True)\n",
    "        d_loss_fake = discriminator(generated_samples, training=True)\n",
    "        d_loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_labels, d_loss_real))\n",
    "        d_loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(fake_labels, d_loss_fake))\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    \n",
    "    grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    if grads and any(g is not None for g in grads):\n",
    "        grads_and_vars = [(g, v) for g, v in zip(grads, discriminator.trainable_variables) if g is not None]\n",
    "        discriminator.optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    # Train generator\n",
    "    with tf.GradientTape() as tape:\n",
    "        noise = tf.random.normal([batch_size, noise_dim])\n",
    "        generated_samples = generator(noise, training=True)\n",
    "        g_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_labels, discriminator(generated_samples, training=True)))\n",
    "    \n",
    "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    if grads and any(g is not None for g in grads):\n",
    "        grads_and_vars = [(g, v) for g, v in zip(grads, generator.trainable_variables) if g is not None]\n",
    "        gan.optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    return d_loss, g_loss\n",
    "\n",
    "def train_gan(epochs, batch_size, data):\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "        real_samples = data[idx]\n",
    "\n",
    "        d_loss, g_loss = train_step(real_samples)\n",
    "\n",
    "      \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch}: Discriminator Loss: {d_loss.numpy()}, Generator Loss: {g_loss.numpy()}\")\n",
    "\n",
    "\n",
    "data = outlier_data\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(epochs=20000, batch_size=128, data=data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "noise = np.random.normal(0, 1, size=(1033,noise_dim))\n",
    "generated_samples = generator.predict(noise)\n",
    "generated_samples = sc.inverse_transform(generated_samples)\n",
    "real_mean = np.mean(un_transformed)\n",
    "real_std = np.std(un_transformed)\n",
    "generated_mean = np.mean(generated_samples)\n",
    "generated_std = np.std(generated_samples)\n",
    "\n",
    "print(f\"Real Data - Mean: {real_mean}, Std: {real_std}\")\n",
    "print(f\"Generated Data - Mean: {generated_mean}, Std: {generated_std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('Generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(normal_data.shape)\n",
    "noise = np.random.normal(0, 1, size=(20333,noise_dim))\n",
    "generator= tf.keras.models.load_model('Generator.h5')\n",
    "\n",
    "generated_samples = generator.predict(noise)\n",
    "generated_samples = sc.inverse_transform(generated_samples)\n",
    "real_mean = np.mean(un_transformed)\n",
    "real_std = np.std(un_transformed)\n",
    "generated_mean = np.mean(generated_samples)\n",
    "generated_std = np.std(generated_samples)\n",
    "\n",
    "print(f\"Real Data - Mean: {real_mean}, Std: {real_std}\")\n",
    "print(f\"Generated Data - Mean: {generated_mean}, Std: {generated_std}\")\n",
    "print(generated_samples)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033, 1)\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "print(un_transformed.shape)\n",
    "noise = np.random.normal(0, 1, size=(1033,noise_dim))\n",
    "generated_samples = generator.predict(noise)\n",
    "generated_samples = sc.inverse_transform(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example datasets (replace these with your actual datasets)\n",
    "actual_data = un_transformed\n",
    "generated_data = generated_samples\n",
    "# Create a KDE plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot KDE for the actual dataset\n",
    "sns.kdeplot(actual_data, label='Actual Data', color='blue', fill=False, alpha=1)\n",
    "\n",
    "# Plot KDE for the generated dataset\n",
    "sns.kdeplot(generated_data, label='Generated Data', color='yellow', fill=False, alpha=0.3)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('KDE Comparison of Actual and Generated Datasets')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: single feature point\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(range(len(generated_samples)), generated_samples, c='blue', marker='o', edgecolor='white', zorder=5)\n",
    "plt.scatter(range(len(sequences)), sequences, c='red', marker='o', edgecolor='black')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Single Feature Points\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest(contamination=0.002, random_state=42)\n",
    "iso_forest.fit(generated_samples)\n",
    "\n",
    "\n",
    "predictions = iso_forest.predict(generated_samples)\n",
    "outliers = np.where(predictions == 1)\n",
    "true_val = np.where(predictions == -1)\n",
    "actual_noise = generated_samples[outliers].reshape(-1, 1)\n",
    "outlier_data = generated_samples[true_val].reshape(-1, 1)\n",
    "print(len(outlier_data))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(range(len(actual_noise)),actual_noise, c='blue', marker='o', edgecolor='black', zorder=5)\n",
    "plt.scatter(range(len(outlier_data)), outlier_data, c='red', marker='o', edgecolor='black')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Single Feature Points\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20644, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset_timeseries.csv\")\n",
    "data = df.groupby(\"Number\")\n",
    "split = {g: d for g, d in data}\n",
    "dataset = split[1].iloc[:, 1:]\n",
    "sequences = dataset['SOPAS'].values.reshape(-1, 1)\n",
    "print(sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actual_noise=pd.DataFrame(generated_samples,columns=[\"SOPAS\"])\n",
    "actual_noise['Number']=0\n",
    "normal_data=pd.DataFrame(sequences,columns=[\"SOPAS\"])\n",
    "normal_data['Number']=1\n",
    "final_dataset=pd.concat([normal_data,actual_noise],ignore_index=True)\n",
    "# Shuffle the combined dataframe\n",
    "final_dataset.to_csv('final_dataset.csv')\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=final_dataset['SOPAS'].values.reshape(-1,1)\n",
    "X=sc.fit_transform(X)\n",
    "Y=final_dataset['Number'].values.reshape(-1,1)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=45)\n",
    "shape=x_train.shape\n",
    "print(x_train,y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,activation='relu', input_dim=1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the classification report dictionary to a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Extract class names\n",
    "class_names = list(report_dict.keys())\n",
    "class_names.remove('accuracy')  # Remove 'accuracy' from the class names list\n",
    "\n",
    "# Create DataFrame from classification report\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df = report_df.reset_index().rename(columns={'index': 'Class'})\n",
    "\n",
    "# Save the classification report to a PDF\n",
    "pdf_filename = 'classification_report.pdf'\n",
    "\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.table(cellText=report_df.values,\n",
    "              colLabels=report_df.columns,\n",
    "              rowLabels=report_df['Class'],\n",
    "              cellLoc='center',\n",
    "              loc='center')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Classification Report\\nAccuracy: {accuracy:.2f}')\n",
    "    \n",
    "    # Save the plot to the PDF\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "print(f'Classification report saved to {pdf_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Noise Classifier.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
